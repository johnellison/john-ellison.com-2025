# AI Transformation Readiness Assessment

This document contains the full set of questions from the AI Transformation Readiness Assessment. It is designed to be shared with stakeholders to gather input before completing the digital assessment.

## Instructions
For each question, select the option that best describes your organization's current state.

---

## 1. Leadership
**Focus:** Executive sponsorship, strategy alignment, and budget commitment.

**1.1 Does your organization have a named executive sponsor or Chief AI Officer?**
- [ ] Yes, with C-suite visibility
- [ ] Yes, but lower in organization
- [ ] Emerging role; not yet formalized
- [ ] No executive ownership

**1.2 Is your AI strategy formally documented and approved by the C-suite?**
- [ ] Yes, with 3-year budget commitment
- [ ] Yes, but budget approved annually
- [ ] In draft; not yet approved
- [ ] No formal strategy

**1.3 Beyond "cost reduction," how are AI success metrics defined?**
- [ ] User adoption, outcome improvement, ROIâ€”all quantified
- [ ] Mix of qualitative and quantitative metrics
- [ ] Primary focus on cost reduction
- [ ] No success metrics defined

**1.4 What % of your C-suite/leadership team actively use AI tools in their own work?**
- [ ] 75%+
- [ ] 50-75%
- [ ] 25-50%
- [ ] <25% or unknown

**1.5 How is AI integrated into your strategic planning process?**
- [ ] Core to annual strategy; discussed quarterly
- [ ] Discussed in strategic review; integrated into some initiatives
- [ ] Pilot projects; not yet strategic
- [ ] Not part of strategic planning

**1.6 What is your organization's AI budget for the next 12 months?**
- [ ] 3-5% of IT budget or dedicated line item
- [ ] 1-3% of IT budget
- [ ] <1% or project-based
- [ ] Minimal/undecided

**1.7 How aligned is AI strategy with overall business goals?**
- [ ] Deeply aligned; AI enables specific competitive advantages
- [ ] Aligned; supports strategic objectives
- [ ] Loosely aligned; opportunistic
- [ ] Unclear connection

---

## 2. Data
**Focus:** Data quality, accessibility, governance, and infrastructure.

**2.1 What percentage of your enterprise data is documented in a data catalog?**
- [ ] 80%+
- [ ] 50-80%
- [ ] 20-50%
- [ ] <20% or none

**2.2 How would you rate your organization's overall data quality?**
- [ ] 80%+ data accurate/complete; automated quality monitoring
- [ ] 60-80% quality; some monitoring
- [ ] 40-60% quality; manual spot checks
- [ ] <40% or unknown

**2.3 Is your data centralized (data warehouse/lake) or siloed?**
- [ ] Centralized in cloud data warehouse/lake
- [ ] Majority centralized; some silos remain
- [ ] Mix of centralized and siloed
- [ ] Primarily siloed across legacy systems

**2.4 Is data ownership clearly assigned (Chief Data Officer, data stewards)?**
- [ ] Yes, formal governance roles and responsibilities
- [ ] Partially; some ownership assigned
- [ ] Emerging; not yet formalized
- [ ] No clear ownership

**2.5 How long does it typically take to get access to needed data?**
- [ ] <1 day; self-service; documented
- [ ] 1-3 days; some friction
- [ ] 1-2 weeks; significant bureaucracy
- [ ] >2 weeks or very difficult

**2.6 Do you have automated data quality monitoring and alerting?**
- [ ] Yes, for all critical data
- [ ] Yes, for most critical data
- [ ] Manual spot checks only
- [ ] No monitoring

**2.7 How is your organization's data governance maturity for regulatory compliance?**
- [ ] Mature framework; documented and enforced
- [ ] Framework in place; inconsistently enforced
- [ ] Emerging framework; gaps remain
- [ ] Minimal/no governance

**2.8 What % of your team's time do analysts spend cleaning/preparing data vs. analysis?**
- [ ] <20% on cleaning; 80%+ on analysis
- [ ] 20-40% on cleaning
- [ ] 40-60% on cleaning
- [ ] >60% on cleaning

---

## 3. Technology
**Focus:** Cloud maturity, MLOps, security, and scalability.

**3.1 What is your organization's primary cloud platform maturity?**
- [ ] Primary workloads on cloud (AWS/Azure/GCP); mature
- [ ] Significant workloads on cloud; maturing
- [ ] Pilot cloud projects; mostly on-premise
- [ ] Primarily on-premise; limited cloud

**3.2 Do you have established CI/CD pipelines with automated testing?**
- [ ] Yes, mature; most deployments automated
- [ ] Partially; some projects have CI/CD
- [ ] Emerging; manual deployments still common
- [ ] Primarily manual deployments

**3.3 Which ML/AI platform have you adopted or are evaluating?**
- [ ] Established platform (Databricks, Vertex AI, SageMaker, etc.)
- [ ] Some platform tooling; not yet standardized
- [ ] Evaluating options; no standardization
- [ ] No dedicated AI/ML platform

**3.4 How does your infrastructure handle data security and access controls?**
- [ ] Role-based access, encryption, audit trails; mature
- [ ] Access controls and encryption in place
- [ ] Basic security; gaps exist
- [ ] Minimal security controls

**3.5 How many integrations exist between your AI/ML systems and operational systems?**
- [ ] 5+ production AI integrations; APIs well-designed
- [ ] 3-4 integrations; some friction
- [ ] 1-2 integrations or pilots only
- [ ] No production AI integrations yet

**3.6 What is your infrastructure readiness for scaling AI (containerization, orchestration)?**
- [ ] Kubernetes/containerization mature; ready to scale
- [ ] Containers in use; some orchestration
- [ ] Exploring containerization
- [ ] Traditional VMs only; not container-ready

---

## 4. Talent
**Focus:** In-house capability, training, and talent retention.

**4.1 Does your organization have in-house data science/ML engineering capability?**
- [ ] Mature team (5+ FTE); mix of seniority
- [ ] Solid team (2-4 FTE); mostly mid-level
- [ ] 1 FTE or junior-heavy team
- [ ] No in-house capability; fully external

**4.2 How many employees have completed AI/ML training in the past 12 months?**
- [ ] 20%+ of workforce
- [ ] 10-20%
- [ ] 5-10%
- [ ] <5%

**4.3 Do you have a Chief Data Officer, Chief AI Officer, or equivalent leadership?**
- [ ] Yes, dedicated role; executive sponsor
- [ ] Yes, but part-time or recent hire
- [ ] Emerging role; not yet formalized
- [ ] No such role

**4.4 How well do business stakeholders understand AI capabilities and limitations?**
- [ ] Strong understanding; realistic expectations
- [ ] Good understanding; mostly realistic
- [ ] Basic understanding; some misconceptions
- [ ] Limited understanding; many misconceptions

**4.5 What is your annual data scientist/ML engineer turnover rate?**
- [ ] <10% (competitive retention)
- [ ] 10-20% (industry average)
- [ ] 20-30% (above industry)
- [ ] >30% (high churn)

**4.6 How does your AI talent acquisition pace compare to organizational needs?**
- [ ] Proactive hiring; meeting demand
- [ ] Hiring ongoing; meeting 70-80% of demand
- [ ] Challenges hiring; 50-70% of positions filled
- [ ] Significant hiring gaps; talent unavailable

**4.7 Do you have structured career development pathways for data science/AI roles?**
- [ ] Yes, clear progression and reskilling programs
- [ ] Emerging pathways; inconsistent
- [ ] Ad-hoc development
- [ ] No structured pathways

---

## 5. Governance
**Focus:** Ethics, bias testing, explainability, and compliance.

**5.1 Does your organization have a documented AI governance policy or framework?**
- [ ] Yes, comprehensive; covers development, deployment, monitoring
- [ ] Yes, but basic; some gaps
- [ ] In development; not yet approved
- [ ] No formal governance

**5.2 Are bias testing and fairness assessments automated for high-risk models?**
- [ ] Yes, all high-risk models tested; automated
- [ ] Most high-risk models tested; some automation
- [ ] Manual testing only; inconsistent
- [ ] No bias testing

**5.3 Can you explain how your organization's AI models make specific decisions?**
- [ ] Yes, all models documented; explainability standards defined
- [ ] Most models explainable; some black-box models exist
- [ ] Limited explainability; documentation gaps
- [ ] Models not explainable; unclear how they work

**5.4 Do you have audit trails and documentation for all AI-generated decisions?**
- [ ] Yes, complete audit trails; logged and monitored
- [ ] Partially; high-risk decisions logged
- [ ] Inconsistent; some documentation gaps
- [ ] No audit trails

**5.5 How prepared is your organization for regulatory compliance in AI (GDPR, CCPA, EU AI Act)?**
- [ ] Mature compliance framework; audit-ready
- [ ] Compliance strategy in place; mostly ready
- [ ] Emerging compliance efforts; gaps remain
- [ ] Minimal compliance preparation

---

## 6. Culture
**Focus:** Employee sentiment, innovation, and psychological safety.

**6.1 How do employees perceive AI in your organization?**
- [ ] Positive; seen as augmentation and opportunity
- [ ] Mostly positive; some concerns
- [ ] Mixed sentiment; significant skepticism
- [ ] Negative; fear and resistance

**6.2 Do employees believe their jobs are at risk due to AI?**
- [ ] No; leadership messaging clear about reskilling
- [ ] Minority concerns; mostly addressed
- [ ] Significant concerns; not fully addressed
- [ ] Yes; widespread job displacement fears

**6.3 Do leaders visibly model AI adoption in their own work?**
- [ ] Yes; 75%+ of leaders actively use AI
- [ ] Many leaders use AI; visible but not universal
- [ ] Some leaders use AI; not widely visible
- [ ] Limited visible leader adoption

**6.4 How would you characterize your organizational culture around innovation?**
- [ ] Strong; failure is acceptable learning
- [ ] Good; experimentation encouraged
- [ ] Mixed; some innovation; risk-averse in places
- [ ] Conservative; risk-averse; limited experimentation

**6.5 Is there psychological safety for employees to admit knowledge gaps about AI?**
- [ ] Yes; strong psychological safety
- [ ] Mostly; some safe spaces
- [ ] Limited; some fear of judgment
- [ ] Low; fear of judgment common
